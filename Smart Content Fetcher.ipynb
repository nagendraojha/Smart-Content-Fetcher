{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ce3ff02f-44e9-47ba-8bf1-f8df5614192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7e027806-a942-4a69-8186-14af4a2e821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install wikipedia requests beautifulsoup4 googlesearch-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "83851916-a120-4184-ba50-3903548b7a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install concurrent-log-handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9f9d4129-8c49-45e9-80cc-ab54ccd2684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import requests\n",
    "from tkinter import Tk, Frame, Entry, Button, Text, Scrollbar, END, Radiobutton, StringVar, Label, messagebox\n",
    "from tkinter.ttk import Progressbar\n",
    "import webbrowser\n",
    "from bs4 import BeautifulSoup\n",
    "import threading\n",
    "from googlesearch import search\n",
    "import re\n",
    "\n",
    "class ContentFetcher:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.setup_gui()\n",
    "        \n",
    "    def setup_gui(self):\n",
    "        self.root.title(\"Smart Content Fetcher\")\n",
    "        self.root.geometry(\"800x600\")\n",
    "        self.root.resizable(True, True)\n",
    "        self.root.configure(bg=\"#f0f2f5\")\n",
    "        \n",
    "        # Main container\n",
    "        main_frame = Frame(self.root, bg=\"#f0f2f5\", padx=20, pady=20)\n",
    "        main_frame.pack(fill=\"both\", expand=True)\n",
    "        \n",
    "        # Search section\n",
    "        search_frame = Frame(main_frame, bg=\"#f0f2f5\")\n",
    "        search_frame.pack(fill=\"x\", pady=(0, 15))\n",
    "        \n",
    "        Label(search_frame, text=\"Search Topic:\", bg=\"#f0f2f5\", font=(\"Arial\", 12)).pack(side=\"left\", padx=(0, 10))\n",
    "        \n",
    "        self.search_entry = Entry(search_frame, font=(\"Arial\", 12), width=40)\n",
    "        self.search_entry.pack(side=\"left\", fill=\"x\", expand=True, padx=(0, 10))\n",
    "        \n",
    "        self.search_btn = Button(search_frame, text=\"Search\", command=self.start_search, \n",
    "                               bg=\"#4CAF50\", fg=\"white\", font=(\"Arial\", 10, \"bold\"))\n",
    "        self.search_btn.pack(side=\"left\")\n",
    "        \n",
    "        # Options frame\n",
    "        options_frame = Frame(main_frame, bg=\"#f0f2f5\")\n",
    "        options_frame.pack(fill=\"x\", pady=(0, 15))\n",
    "        \n",
    "        self.content_type = StringVar(value=\"verified\")\n",
    "        \n",
    "        Radiobutton(options_frame, text=\"Verified Summary\", variable=self.content_type, \n",
    "                   value=\"verified\", bg=\"#f0f2f5\", font=(\"Arial\", 10)).pack(side=\"left\", padx=(0, 10))\n",
    "        Radiobutton(options_frame, text=\"Wikipedia Summary\", variable=self.content_type, \n",
    "                   value=\"wiki_summary\", bg=\"#f0f2f5\", font=(\"Arial\", 10)).pack(side=\"left\", padx=(0, 10))\n",
    "        Radiobutton(options_frame, text=\"Full Content\", variable=self.content_type, \n",
    "                   value=\"full\", bg=\"#f0f2f5\", font=(\"Arial\", 10)).pack(side=\"left\", padx=(0, 10))\n",
    "        \n",
    "        # Results section\n",
    "        results_frame = Frame(main_frame, bg=\"#ffffff\", bd=2, relief=\"solid\")\n",
    "        results_frame.pack(fill=\"both\", expand=True)\n",
    "        \n",
    "        self.results_text = Text(results_frame, font=(\"Arial\", 11), wrap=\"word\", \n",
    "                                padx=10, pady=10, state=\"disabled\")\n",
    "        scrollbar = Scrollbar(results_frame, command=self.results_text.yview)\n",
    "        self.results_text.configure(yscrollcommand=scrollbar.set)\n",
    "        \n",
    "        scrollbar.pack(side=\"right\", fill=\"y\")\n",
    "        self.results_text.pack(fill=\"both\", expand=True)\n",
    "        \n",
    "        # Progress bar\n",
    "        self.progress = Progressbar(main_frame, mode=\"indeterminate\")\n",
    "        self.progress.pack(fill=\"x\", pady=(10, 0))\n",
    "        \n",
    "        # Bottom buttons\n",
    "        bottom_frame = Frame(main_frame, bg=\"#f0f2f5\")\n",
    "        bottom_frame.pack(fill=\"x\", pady=(15, 0))\n",
    "        \n",
    "        Button(bottom_frame, text=\"Search Web\", command=self.search_web, \n",
    "              bg=\"#2196F3\", fg=\"white\", font=(\"Arial\", 10)).pack(side=\"left\", padx=(0, 10))\n",
    "        Button(bottom_frame, text=\"Clear\", command=self.clear_results, \n",
    "              bg=\"#f44336\", fg=\"white\", font=(\"Arial\", 10)).pack(side=\"left\")\n",
    "        \n",
    "    def start_search(self):\n",
    "        query = self.search_entry.get().strip()\n",
    "        if not query:\n",
    "            messagebox.showwarning(\"Warning\", \"Please enter a search term\")\n",
    "            return\n",
    "            \n",
    "        self.progress.start()\n",
    "        self.search_btn.config(state=\"disabled\")\n",
    "        self.results_text.config(state=\"normal\")\n",
    "        self.results_text.delete(1.0, END)\n",
    "        self.results_text.insert(END, \"Searching... Please wait.\")\n",
    "        self.results_text.config(state=\"disabled\")\n",
    "        \n",
    "        # Run search in separate thread to prevent GUI freezing\n",
    "        threading.Thread(target=self.perform_search, args=(query,), daemon=True).start()\n",
    "        \n",
    "    def perform_search(self, query):\n",
    "        try:\n",
    "            content_type = self.content_type.get()\n",
    "            \n",
    "            if content_type == \"verified\":\n",
    "                result = self.get_verified_content(query)\n",
    "            elif content_type == \"wiki_summary\":\n",
    "                result = self.get_wikipedia_summary(query)\n",
    "            elif content_type == \"full\":\n",
    "                result = self.get_full_content(query)\n",
    "                \n",
    "            self.display_results(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.display_results(f\"Error: {str(e)}\")\n",
    "        finally:\n",
    "            self.progress.stop()\n",
    "            self.search_btn.config(state=\"normal\")\n",
    "    \n",
    "    def get_verified_content(self, query):\n",
    "        \"\"\"Get content from multiple sources and verify consistency\"\"\"\n",
    "        try:\n",
    "            # Get results from multiple sources\n",
    "            wiki_summary = self.get_wikipedia_summary(query)\n",
    "            web_content = self.search_web_content(query)\n",
    "            \n",
    "            # Simple verification - check if key terms appear in both\n",
    "            key_terms = set(query.lower().split())\n",
    "            wiki_terms = set(wiki_summary.lower().split())\n",
    "            web_terms = set(web_content.lower().split())\n",
    "            \n",
    "            common_terms = key_terms.intersection(wiki_terms).intersection(web_terms)\n",
    "            \n",
    "            if len(common_terms) / len(key_terms) > 0.5:  # If >50% terms match\n",
    "                # Return the more detailed version\n",
    "                return web_content if len(web_content) > len(wiki_summary) else wiki_summary\n",
    "            else:\n",
    "                return f\"Warning: Sources disagree on this topic.\\n\\nWikipedia says:\\n{wiki_summary}\\n\\nWeb sources say:\\n{web_content}\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"Verification failed: {str(e)}\"\n",
    "    \n",
    "    def get_wikipedia_summary(self, query):\n",
    "        try:\n",
    "            wikipedia.set_lang(\"en\")\n",
    "            return wikipedia.summary(query, sentences=5)\n",
    "        except wikipedia.exceptions.DisambiguationError as e:\n",
    "            return f\"Multiple matches found. Please be more specific. Options: {', '.join(e.options[:5])}...\"\n",
    "        except wikipedia.exceptions.PageError:\n",
    "            return \"No Wikipedia page found for this topic.\"\n",
    "        except Exception as e:\n",
    "            return f\"Error fetching Wikipedia summary: {str(e)}\"\n",
    "    \n",
    "    def search_web_content(self, query):\n",
    "        try:\n",
    "            # Get top 3 relevant URLs from Google\n",
    "            search_results = list(search(query, num=3, stop=3, pause=1))\n",
    "            \n",
    "            contents = []\n",
    "            for url in search_results:\n",
    "                try:\n",
    "                    content = self.scrape_website(url)\n",
    "                    if content:\n",
    "                        contents.append(f\"From {url}:\\n{content}\\n\\n\")\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            return \"\\n\".join(contents) if contents else \"No web content found.\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error searching web: {str(e)}\"\n",
    "    \n",
    "    def scrape_website(self, url):\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "            }\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Remove unwanted elements\n",
    "            for element in soup(['script', 'style', 'nav', 'footer', 'iframe']):\n",
    "                element.decompose()\n",
    "                \n",
    "            # Get text and clean it up\n",
    "            text = soup.get_text()\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            \n",
    "            return text[:2000] + \"...\" if len(text) > 2000 else text\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Could not retrieve content from {url}: {str(e)}\"\n",
    "    \n",
    "    def get_full_content(self, query):\n",
    "        try:\n",
    "            wikipedia.set_lang(\"en\")\n",
    "            page = wikipedia.page(query)\n",
    "            return page.content[:5000] + \"...\" if len(page.content) > 5000 else page.content\n",
    "        except Exception as e:\n",
    "            return f\"Error fetching full content: {str(e)}\"\n",
    "    \n",
    "    def display_results(self, text):\n",
    "        self.results_text.config(state=\"normal\")\n",
    "        self.results_text.delete(1.0, END)\n",
    "        self.results_text.insert(END, text)\n",
    "        self.results_text.config(state=\"disabled\")\n",
    "    \n",
    "    def search_web(self):\n",
    "        query = self.search_entry.get().strip()\n",
    "        if not query:\n",
    "            messagebox.showwarning(\"Warning\", \"Please enter a search term\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            webbrowser.open_new_tab(f\"https://www.google.com/search?q={query.replace(' ', '+')}\")\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Could not open browser: {str(e)}\")\n",
    "    \n",
    "    def clear_results(self):\n",
    "        self.results_text.config(state=\"normal\")\n",
    "        self.results_text.delete(1.0, END)\n",
    "        self.results_text.config(state=\"disabled\")\n",
    "        self.search_entry.delete(0, END)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = Tk()\n",
    "    app = ContentFetcher(root)\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ec035-8d69-4dd7-b153-8ca4cfa3eb0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "55548e94-9a93-461b-b570-ff926b153bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import requests\n",
    "from tkinter import Tk, Frame, Entry, Button, Text, Scrollbar, END, Radiobutton, StringVar, Label, messagebox\n",
    "from tkinter.ttk import Progressbar\n",
    "import webbrowser\n",
    "from bs4 import BeautifulSoup\n",
    "import threading\n",
    "from googlesearch import search\n",
    "import re\n",
    "import concurrent.futures\n",
    "\n",
    "class ContentFetcher:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.setup_gui()\n",
    "        \n",
    "    def setup_gui(self):\n",
    "        self.root.title(\"Smart Content Fetcher\")\n",
    "        self.root.geometry(\"900x700\")\n",
    "        self.root.resizable(True, True)\n",
    "        self.root.configure(bg=\"#f0f2f5\")\n",
    "        \n",
    "        # Main container\n",
    "        main_frame = Frame(self.root, bg=\"#f0f2f5\", padx=20, pady=20)\n",
    "        main_frame.pack(fill=\"both\", expand=True)\n",
    "        \n",
    "        # Search section\n",
    "        search_frame = Frame(main_frame, bg=\"#f0f2f5\")\n",
    "        search_frame.pack(fill=\"x\", pady=(0, 15))\n",
    "        \n",
    "        Label(search_frame, text=\"Search Topic:\", bg=\"#f0f2f5\", font=(\"Arial\", 12)).pack(side=\"left\", padx=(0, 10))\n",
    "        \n",
    "        self.search_entry = Entry(search_frame, font=(\"Arial\", 12), width=40)\n",
    "        self.search_entry.pack(side=\"left\", fill=\"x\", expand=True, padx=(0, 10))\n",
    "        \n",
    "        self.search_btn = Button(search_frame, text=\"Search\", command=self.start_search, \n",
    "                               bg=\"#4CAF50\", fg=\"white\", font=(\"Arial\", 10, \"bold\"))\n",
    "        self.search_btn.pack(side=\"left\")\n",
    "        \n",
    "        # Options frame\n",
    "        options_frame = Frame(main_frame, bg=\"#f0f2f5\")\n",
    "        options_frame.pack(fill=\"x\", pady=(0, 15))\n",
    "        \n",
    "        self.content_type = StringVar(value=\"verified\")\n",
    "        \n",
    "        Radiobutton(options_frame, text=\"Verified Summary\", variable=self.content_type, \n",
    "                   value=\"verified\", bg=\"#f0f2f5\", font=(\"Arial\", 10)).pack(side=\"left\", padx=(0, 10))\n",
    "        Radiobutton(options_frame, text=\"Wikipedia Summary\", variable=self.content_type, \n",
    "                   value=\"wiki_summary\", bg=\"#f0f2f5\", font=(\"Arial\", 10)).pack(side=\"left\", padx=(0, 10))\n",
    "        Radiobutton(options_frame, text=\"Full Content\", variable=self.content_type, \n",
    "                   value=\"full\", bg=\"#f0f2f5\", font=(\"Arial\", 10)).pack(side=\"left\", padx=(0, 10))\n",
    "        \n",
    "        # Results section\n",
    "        results_frame = Frame(main_frame, bg=\"#ffffff\", bd=2, relief=\"solid\")\n",
    "        results_frame.pack(fill=\"both\", expand=True)\n",
    "        \n",
    "        self.results_text = Text(results_frame, font=(\"Arial\", 11), wrap=\"word\", \n",
    "                                padx=10, pady=10, state=\"disabled\")\n",
    "        scrollbar = Scrollbar(results_frame, command=self.results_text.yview)\n",
    "        self.results_text.configure(yscrollcommand=scrollbar.set)\n",
    "        \n",
    "        scrollbar.pack(side=\"right\", fill=\"y\")\n",
    "        self.results_text.pack(fill=\"both\", expand=True)\n",
    "        \n",
    "        # Progress bar\n",
    "        self.progress = Progressbar(main_frame, mode=\"indeterminate\")\n",
    "        self.progress.pack(fill=\"x\", pady=(10, 0))\n",
    "        \n",
    "        # Bottom buttons\n",
    "        bottom_frame = Frame(main_frame, bg=\"#f0f2f5\")\n",
    "        bottom_frame.pack(fill=\"x\", pady=(15, 0))\n",
    "        \n",
    "        Button(bottom_frame, text=\"Search Web\", command=self.search_web, \n",
    "              bg=\"#2196F3\", fg=\"white\", font=(\"Arial\", 10)).pack(side=\"left\", padx=(0, 10))\n",
    "        Button(bottom_frame, text=\"Clear\", command=self.clear_results, \n",
    "              bg=\"#f44336\", fg=\"white\", font=(\"Arial\", 10)).pack(side=\"left\")\n",
    "        \n",
    "    def start_search(self):\n",
    "        query = self.search_entry.get().strip()\n",
    "        if not query:\n",
    "            messagebox.showwarning(\"Warning\", \"Please enter a search term\")\n",
    "            return\n",
    "            \n",
    "        self.progress.start()\n",
    "        self.search_btn.config(state=\"disabled\")\n",
    "        self.results_text.config(state=\"normal\")\n",
    "        self.results_text.delete(1.0, END)\n",
    "        self.results_text.insert(END, \"Searching... Please wait.\")\n",
    "        self.results_text.config(state=\"disabled\")\n",
    "        \n",
    "        # Run search in separate thread to prevent GUI freezing\n",
    "        threading.Thread(target=self.perform_search, args=(query,), daemon=True).start()\n",
    "        \n",
    "    def perform_search(self, query):\n",
    "        try:\n",
    "            content_type = self.content_type.get()\n",
    "            \n",
    "            if content_type == \"verified\":\n",
    "                result = self.get_verified_content(query)\n",
    "            elif content_type == \"wiki_summary\":\n",
    "                result = self.get_wikipedia_summary(query)\n",
    "            elif content_type == \"full\":\n",
    "                result = self.get_full_content(query)\n",
    "                \n",
    "            self.display_results(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.display_results(f\"Error: {str(e)}\")\n",
    "        finally:\n",
    "            self.progress.stop()\n",
    "            self.search_btn.config(state=\"normal\")\n",
    "    \n",
    "    def get_verified_content(self, query):\n",
    "        \"\"\"Get content from multiple sources and verify consistency\"\"\"\n",
    "        try:\n",
    "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                # Get results from multiple sources concurrently\n",
    "                wiki_future = executor.submit(self.get_wikipedia_summary, query)\n",
    "                web_future = executor.submit(self.search_web_content, query)\n",
    "                \n",
    "                wiki_summary = wiki_future.result()\n",
    "                web_content = web_future.result()\n",
    "            \n",
    "            # Improved verification logic\n",
    "            if \"Error\" in wiki_summary or \"Error\" in web_content:\n",
    "                return f\"Wikipedia:\\n{wiki_summary}\\n\\nWeb Results:\\n{web_content}\"\n",
    "            \n",
    "            # Check similarity using more sophisticated method\n",
    "            similarity_score = self.calculate_similarity(wiki_summary, web_content)\n",
    "            \n",
    "            if similarity_score > 0.6:  # If sources mostly agree\n",
    "                # Combine the best parts of both\n",
    "                combined = self.combine_contents(wiki_summary, web_content)\n",
    "                return f\"Verified Information:\\n\\n{combined}\"\n",
    "            else:\n",
    "                return f\"Sources show different information:\\n\\nWikipedia:\\n{wiki_summary}\\n\\nWeb Results:\\n{web_content}\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"Verification failed: {str(e)}\"\n",
    "    \n",
    "    def calculate_similarity(self, text1, text2):\n",
    "        \"\"\"Calculate a simple similarity score between two texts\"\"\"\n",
    "        words1 = set(re.findall(r'\\w+', text1.lower()))\n",
    "        words2 = set(re.findall(r'\\w+', text2.lower()))\n",
    "        \n",
    "        common_words = words1.intersection(words2)\n",
    "        total_words = words1.union(words2)\n",
    "        \n",
    "        return len(common_words) / len(total_words) if total_words else 0\n",
    "    \n",
    "    def combine_contents(self, wiki_content, web_content):\n",
    "        \"\"\"Combine the best parts from both sources\"\"\"\n",
    "        # Take first 3 sentences from Wikipedia\n",
    "        wiki_sentences = re.split(r'(?<=[.!?])\\s+', wiki_content)\n",
    "        combined = ' '.join(wiki_sentences[:3]) + \"\\n\\n\"\n",
    "        \n",
    "        # Add additional unique information from web content\n",
    "        web_sentences = re.split(r'(?<=[.!?])\\s+', web_content)\n",
    "        for sentence in web_sentences:\n",
    "            if sentence not in wiki_content and len(sentence.split()) > 5:\n",
    "                combined += sentence + \" \"\n",
    "                if len(combined.split()) > 150:  # Limit to reasonable length\n",
    "                    break\n",
    "                    \n",
    "        return combined.strip()\n",
    "    \n",
    "    def get_wikipedia_summary(self, query):\n",
    "        try:\n",
    "            wikipedia.set_lang(\"en\")\n",
    "            return wikipedia.summary(query, sentences=5)\n",
    "        except wikipedia.exceptions.DisambiguationError as e:\n",
    "            return f\"Multiple matches found. Please be more specific. Options: {', '.join(e.options[:5])}...\"\n",
    "        except wikipedia.exceptions.PageError:\n",
    "            return \"No Wikipedia page found for this topic.\"\n",
    "        except Exception as e:\n",
    "            return f\"Error fetching Wikipedia summary: {str(e)}\"\n",
    "    \n",
    "    def search_web_content(self, query):\n",
    "        try:\n",
    "            # Get top 5 relevant URLs from Google\n",
    "            search_results = list(search(query, num=5, stop=5, pause=1))\n",
    "            \n",
    "            contents = []\n",
    "            for url in search_results[:3]:  # Only use top 3 to be faster\n",
    "                try:\n",
    "                    content = self.scrape_website(url)\n",
    "                    if content and not any(err in content for err in [\"Error\", \"Could not retrieve\"]):\n",
    "                        contents.append(content)\n",
    "                        if len(contents) >= 2:  # Stop after getting 2 good sources\n",
    "                            break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if not contents:\n",
    "                return \"No reliable web content found.\"\n",
    "                \n",
    "            # Combine the best parts of the web results\n",
    "            return self.combine_web_contents(contents)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error searching web: {str(e)}\"\n",
    "    \n",
    "    def combine_web_contents(self, contents):\n",
    "        \"\"\"Combine multiple web contents intelligently\"\"\"\n",
    "        if len(contents) == 1:\n",
    "            return contents[0]\n",
    "            \n",
    "        # Find common sentences\n",
    "        all_sentences = []\n",
    "        for content in contents:\n",
    "            sentences = re.split(r'(?<=[.!?])\\s+', content)\n",
    "            all_sentences.append(set(sentences))\n",
    "            \n",
    "        common_sentences = set.intersection(*all_sentences)\n",
    "        \n",
    "        # Build result starting with common sentences\n",
    "        result = []\n",
    "        if common_sentences:\n",
    "            result.append(\"Common information from multiple sources:\")\n",
    "            result.extend(common_sentences)\n",
    "            result.append(\"\\nAdditional information:\")\n",
    "        \n",
    "        # Add unique information\n",
    "        for sentences in all_sentences:\n",
    "            unique = sentences - common_sentences\n",
    "            if unique:\n",
    "                result.extend(unique)\n",
    "                break  # Just add from one source to avoid duplication\n",
    "                \n",
    "        return ' '.join(result)\n",
    "    \n",
    "    def scrape_website(self, url):\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "            }\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Remove unwanted elements\n",
    "            for element in soup(['script', 'style', 'nav', 'footer', 'iframe', 'header']):\n",
    "                element.decompose()\n",
    "                \n",
    "            # Get text from likely content-containing elements\n",
    "            content_elements = soup.find_all(['article', 'main', 'div[class*=\"content\"]', 'p'])\n",
    "            text = ' '.join([elem.get_text() for elem in content_elements])\n",
    "            \n",
    "            # Clean up text\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            text = re.sub(r'\\[[^\\]]+\\]', '', text)  # Remove citations like [1], [2]\n",
    "            \n",
    "            if not text or len(text.split()) < 20:  # Skip if too short\n",
    "                return None\n",
    "                \n",
    "            return text[:3000]  # Return first 3000 characters\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"Could not retrieve content from {url}: {str(e)}\"\n",
    "    \n",
    "    def get_full_content(self, query):\n",
    "        try:\n",
    "            wikipedia.set_lang(\"en\")\n",
    "            page = wikipedia.page(query)\n",
    "            return page.content  # Return full content without truncation\n",
    "        except Exception as e:\n",
    "            return f\"Error fetching full content: {str(e)}\"\n",
    "    \n",
    "    def display_results(self, text):\n",
    "        self.results_text.config(state=\"normal\")\n",
    "        self.results_text.delete(1.0, END)\n",
    "        self.results_text.insert(END, text)\n",
    "        self.results_text.config(state=\"disabled\")\n",
    "    \n",
    "    def search_web(self):\n",
    "        query = self.search_entry.get().strip()\n",
    "        if not query:\n",
    "            messagebox.showwarning(\"Warning\", \"Please enter a search term\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            webbrowser.open_new_tab(f\"https://www.google.com/search?q={query.replace(' ', '+')}\")\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Could not open browser: {str(e)}\")\n",
    "    \n",
    "    def clear_results(self):\n",
    "        self.results_text.config(state=\"normal\")\n",
    "        self.results_text.delete(1.0, END)\n",
    "        self.results_text.config(state=\"disabled\")\n",
    "        self.search_entry.delete(0, END)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = Tk()\n",
    "    app = ContentFetcher(root)\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d379ff98-19fc-44e3-9a7c-faa39f1bd6eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcca407f-5912-4de1-bfdc-476ddd75d31c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1631e392-a54e-4f9b-ba41-0e622ed54c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c04d46-ed6f-452f-b642-814aec81ab95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbdf252-5ded-402e-8bd7-24b45e1f39a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d5c2a0-2061-40a1-b41b-b05faf56b6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7158d799-c66f-4e97-8c32-8dd7d16481be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3965dae4-02ed-40ac-8b81-4dbdaa3977b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5922b6-dc75-4ab4-9ae2-fc6465f5d66f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
